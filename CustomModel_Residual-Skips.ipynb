{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Custom CNN model with residual skips"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JiiM6KzKxb7n",
        "outputId": "3109ab24-47e9-4984-a530-4999aef87d16"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZCUQzWYvy9cL",
        "outputId": "d1092dbb-3647-4daf-8814-a52137128c32"
      },
      "outputs": [],
      "source": [
        "# %cd /gdrive/My Drive/AN2DL/leaves/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l3FoTyRa9pLu"
      },
      "source": [
        "### Import libraries"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f_sOaV1Y8NsL",
        "outputId": "3300ad7b-edc3-4123-ded4-ef76bc10df7f"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "tfk = tf.keras\n",
        "tfkl = tf.keras.layers\n",
        "\n",
        "print(tf.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pLb-N5JzUUQS"
      },
      "source": [
        "### Set seed for reproducibility"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C7HYua8HUHIj"
      },
      "outputs": [],
      "source": [
        "# Random seed for reproducibility\n",
        "seed = 0xdeadbeef\n",
        "\n",
        "random.seed(seed)\n",
        "os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "np.random.seed(seed)\n",
        "tf.random.set_seed(seed)\n",
        "tf.compat.v1.set_random_seed(seed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MvjjNtBV_jBQ"
      },
      "source": [
        "# Dataset info\n",
        "\n",
        "## Leaves\n",
        "\n",
        "Classes: `['Apple','Blueberry','Cherry','Corn','Grape','Orange','Peach','Pepper','Potato','Raspberry','Soybean','Squash','Strawberry','Tomato']`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jSmn5M8_PyJ1"
      },
      "outputs": [],
      "source": [
        "# Dataset folders \n",
        "dataset_dir = 'output'\n",
        "training_dir = os.path.join(dataset_dir, 'train')\n",
        "validation_dir = os.path.join(dataset_dir, 'val')\n",
        "test_dir = os.path.join(dataset_dir, 'test')\n",
        "model_dir = 'models'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-UrdOzQ2-Jqe"
      },
      "outputs": [],
      "source": [
        "# Plot example images from dataset\n",
        "labels = ['Apple','Blueberry','Cherry','Corn','Grape','Orange','Peach','Pepper','Potato','Raspberry','Soybean','Squash','Strawberry','Tomato']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DcWi9Iat96uk",
        "outputId": "f28c99dc-b63d-47ec-ff5e-9278780ef345"
      },
      "outputs": [],
      "source": [
        "# Images are divided into folders, one for each class. \n",
        "# If the images are organized in such a way, we can exploit the \n",
        "# ImageDataGenerator to read them from disk.\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "def simple_gen():\n",
        "    return ImageDataGenerator(\n",
        "            dtype=tf.float32,\n",
        "        )\n",
        "\n",
        "gen = simple_gen()\n",
        "\n",
        "input_size = (256, 256)\n",
        "input_shape = (256, 256, 3)\n",
        "batch_size = 4 # Small batch size due to HW limitations\n",
        "\n",
        "train_gen = gen.flow_from_directory(\n",
        "    directory=training_dir,\n",
        "    target_size=input_size,\n",
        "    color_mode=\"rgb\",\n",
        "    classes=None,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,\n",
        ")\n",
        "\n",
        "val_gen = gen.flow_from_directory(\n",
        "    directory=validation_dir,\n",
        "    target_size=input_size,\n",
        "    color_mode=\"rgb\",\n",
        "    classes=None,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,\n",
        "    seed=seed\n",
        ")\n",
        "\n",
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "\n",
        "augment_image = tf.keras.Sequential([\n",
        "    tfkl.RandomFlip(\"horizontal_and_vertical\"),\n",
        "    # tfkl.RandomContrast(0.123),\n",
        "    # tfkl.RandomZoom(0.2, 0.2, fill_mode=\"constant\"),\n",
        "    tfkl.RandomRotation(0.2, fill_mode=\"constant\"),\n",
        "    tfkl.RandomTranslation(0.2, 0.2, fill_mode=\"constant\"),\n",
        "])\n",
        "\n",
        "\n",
        "train_ds = tf.data.Dataset.from_generator(\n",
        "    lambda: train_gen,\n",
        "    output_signature=(\n",
        "         tf.TensorSpec(shape=(None, 256, 256, 3), dtype=tf.float32),\n",
        "         tf.TensorSpec(shape=(None, len(labels)), dtype=tf.float32))\n",
        ")\n",
        "train_ds = train_ds.shuffle(256 // batch_size, reshuffle_each_iteration=True)\n",
        "train_ds = train_ds.repeat()\n",
        "train_ds = train_ds.map(\n",
        "    lambda x, y: (augment_image(x, training=True), y),\n",
        "    num_parallel_calls=AUTOTUNE,\n",
        "    deterministic=False\n",
        ")\n",
        "train_ds = train_ds.prefetch(buffer_size=64)\n",
        "\n",
        "val_ds = tf.data.Dataset.from_generator(\n",
        "    lambda: val_gen,\n",
        "    output_signature=(\n",
        "         tf.TensorSpec(shape=(None, 256, 256, 3), dtype=tf.float32),\n",
        "         tf.TensorSpec(shape=(None, len(labels)), dtype=tf.float32))\n",
        ")\n",
        "val_ds = val_ds.repeat().prefetch(buffer_size=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wothhodRGqqo",
        "outputId": "1e8ac202-6809-4461-a00a-7a99fa6ea0bc"
      },
      "outputs": [],
      "source": [
        "print(\"Assigned labels\")\n",
        "print(train_gen.class_indices)\n",
        "print(\"Target classes\")\n",
        "print(train_gen.classes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5D5ln0cHVL2b"
      },
      "outputs": [],
      "source": [
        "def get_next_batch(generator):\n",
        "  image, target = next(generator)\n",
        "\n",
        "  print(\"(Input) image shape:\", image.shape)\n",
        "  print(\"Target shape:\",target.shape)\n",
        "\n",
        "  # Visualize only the first sample\n",
        "  image = image[0]\n",
        "  target = target[0]\n",
        "  target_idx = np.argmax(target)\n",
        "  print(image[128, 128, :])\n",
        "  print(\"Categorical label:\", target)\n",
        "  print(\"Label:\", target_idx)\n",
        "  print(\"Class name:\", labels[target_idx])\n",
        "  fig = plt.figure(figsize=(6, 4))\n",
        "  plt.imshow(image / 255)\n",
        "\n",
        "  return image, target\n",
        "  \n",
        "  # Get a sample from dataset and show info\n",
        "_ = get_next_batch(train_ds.as_numpy_iterator())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KXmw4F0wlY0h"
      },
      "source": [
        "### CNN model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PkccDbv-bzKr"
      },
      "outputs": [],
      "source": [
        "def build_model(input_shape):\n",
        "    inputs = tfk.Input(shape=input_shape)\n",
        "\n",
        "    x = inputs\n",
        "\n",
        "    for d in [32, 64, 128, 256, 512]:\n",
        "        x = tfkl.BatchNormalization()(x)\n",
        "\n",
        "        x = tfkl.Conv2D(d, 1, padding=\"same\")(x)\n",
        "        x = tfkl.ELU()(x)\n",
        "        \n",
        "        r = x\n",
        "\n",
        "        x = tfkl.SeparableConv2D(d, 3, padding=\"same\")(x)\n",
        "        x = tfkl.SeparableConv2D(d, 3, padding=\"same\")(x)\n",
        "        x = tfkl.ELU()(x)\n",
        "        x = tfkl.SeparableConv2D(d, 3, padding=\"same\")(x)\n",
        "        x = tfkl.SeparableConv2D(d, 3, padding=\"same\")(x)\n",
        "        x = tfkl.ELU()(x)\n",
        "\n",
        "        x = tfkl.Add()([x, r])\n",
        "\n",
        "        x = tfkl.BatchNormalization()(x)\n",
        "        x = tfkl.Conv2D(d, 3)(x)\n",
        "        x = tfkl.ELU()(x)\n",
        "        x = tfkl.Conv2D(d, 3)(x)\n",
        "        x = tfkl.ELU()(x)\n",
        "\n",
        "        x = tfkl.MaxPooling2D(2)(x)\n",
        "\n",
        "    x = tfkl.Flatten()(x)\n",
        "    x = tfkl.BatchNormalization()(x)\n",
        "\n",
        "    x = tfk.Sequential([\n",
        "        tfkl.Dense(\n",
        "            256, \n",
        "            activation='relu'),\n",
        "        tfkl.Dropout(0.2),\n",
        "        \n",
        "        tfkl.Dense(\n",
        "            128, \n",
        "            activation='relu'),\n",
        "        tfkl.Dropout(0.2),\n",
        "\n",
        "        tfkl.Dense(\n",
        "            64, \n",
        "            activation='relu'),\n",
        "        tfkl.Dropout(0.1),\n",
        "\n",
        "        tfkl.Dense(\n",
        "            len(labels), \n",
        "            activation='softmax',)\n",
        "    ])(x)\n",
        "\n",
        "    outputs = x\n",
        "    \n",
        "    # Connect input and output through the Model class\n",
        "    model = tfk.Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "    # Return the model\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6qbixQiprQl_"
      },
      "outputs": [],
      "source": [
        "# Utility function to create folders and callbacks for training\n",
        "from datetime import datetime\n",
        "\n",
        "def create_folders_and_callbacks(model_name):\n",
        "\n",
        "  exps_dir = os.path.join('experiments')\n",
        "  if not os.path.exists(exps_dir):\n",
        "      os.makedirs(exps_dir)\n",
        "\n",
        "  now = datetime.now().strftime('%b%d_%H-%M-%S')\n",
        "\n",
        "  exp_dir = os.path.join(exps_dir, model_name + '_' + str(now))\n",
        "  if not os.path.exists(exp_dir):\n",
        "      os.makedirs(exp_dir)\n",
        "      \n",
        "  callbacks = []\n",
        "\n",
        "  # Model checkpoint\n",
        "  # ----------------\n",
        "  ckpt_dir = os.path.join(exp_dir, 'ckpts')\n",
        "  if not os.path.exists(ckpt_dir):\n",
        "      os.makedirs(ckpt_dir)\n",
        "\n",
        "  ckpt_callback = tf.keras.callbacks.ModelCheckpoint(filepath=os.path.join(ckpt_dir, 'cp'),\n",
        "                                                     save_weights_only=True,\n",
        "                                                     save_best_only=True)\n",
        "  callbacks.append(ckpt_callback)\n",
        "\n",
        "  # Weight decay\n",
        "  # ------------\n",
        "\n",
        "  wd_callback = tfk.callbacks.ReduceLROnPlateau(\"val_loss\", 0.1, patience=10)\n",
        "  callbacks.append(wd_callback)\n",
        "\n",
        "  # Visualize Learning on Tensorboard\n",
        "  # ---------------------------------\n",
        "  tb_dir = os.path.join(exp_dir, 'tb_logs')\n",
        "  if not os.path.exists(tb_dir):\n",
        "      os.makedirs(tb_dir)\n",
        "      \n",
        "  tb_callback = tf.keras.callbacks.TensorBoard(log_dir=tb_dir,\n",
        "                                               profile_batch=0,\n",
        "                                               histogram_freq=1)\n",
        "  callbacks.append(tb_callback)\n",
        "\n",
        "  # Early Stopping\n",
        "  # --------------\n",
        "  es_callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
        "  callbacks.append(es_callback)\n",
        "\n",
        "  return callbacks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NwsjIp_crOKq",
        "outputId": "31d21d92-f85c-4b41-8e53-a526f4937070"
      },
      "outputs": [],
      "source": [
        "model_name = 'SpongeNetV2'\n",
        "model_path = os.path.join(model_dir, model_name)\n",
        "weights_path = os.path.join(model_dir, \"weights\", model_name, model_name)\n",
        "\n",
        "tfk.backend.clear_session()\n",
        "\n",
        "model = build_model(input_shape)\n",
        "# model = tfk.models.load_model(model_path)\n",
        "# model.load_weights(weights_path)\n",
        "\n",
        "callbacks = create_folders_and_callbacks(model_name=model_name)\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hln5Up0XtOs9",
        "outputId": "7ce8573b-ad47-4494-b737-08bef87128bd"
      },
      "outputs": [],
      "source": [
        "model.compile(loss=tfk.losses.CategoricalCrossentropy(), optimizer=tfk.optimizers.Adam(0.02, epsilon=0.01), metrics='accuracy')\n",
        "\n",
        "epochs = 50\n",
        "\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(\n",
        "    x = train_ds,\n",
        "    steps_per_epoch=500, # Limiting steps per epoch to better check the metrics\n",
        "    epochs = epochs,\n",
        "    validation_data = val_ds,\n",
        "    validation_steps = 50,\n",
        "    callbacks = callbacks,\n",
        ").history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U6m-yQ1bug3o"
      },
      "outputs": [],
      "source": [
        "# Save best epoch model\n",
        "now = datetime.now().strftime('%b%d_%H-%M-%S')\n",
        "\n",
        "model.save_weights(weights_path + '_' + str(now))\n",
        "model.save(model_path + '_' + str(now))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_batch_size = 4\n",
        "test_data_gen = simple_gen()\n",
        "test_gen = test_data_gen.flow_from_directory(\n",
        "    directory=test_dir,\n",
        "    target_size=input_size,\n",
        "    color_mode=\"rgb\",\n",
        "    classes=None,\n",
        "    batch_size=test_batch_size,\n",
        "    shuffle=False,\n",
        "    seed=seed\n",
        ")\n",
        "\n",
        "model.compile(loss=tfk.losses.CategoricalCrossentropy(), metrics='accuracy')\n",
        "model.evaluate(test_gen)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_l = len(test_gen) * test_batch_size\n",
        "print(test_l)\n",
        "predictions = np.zeros((test_l,))\n",
        "y_test = np.zeros((test_l,))\n",
        "\n",
        "s = 0\n",
        "for X, y in test_gen:\n",
        "    y_idx = np.argmax(y, axis=-1)\n",
        "    p_idx = np.argmax(model.predict(X), axis=-1)\n",
        "    l = len(y_idx)\n",
        "    if s + l >= test_l:\n",
        "        break\n",
        "    y_test[s:s + l] = y_idx\n",
        "    predictions[s:s + l] = p_idx\n",
        "    s += l\n",
        "\n",
        "predictions = predictions[:s]\n",
        "y_test = y_test[:s]\n",
        "predictions.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "cm = confusion_matrix(y_test, predictions)\n",
        "\n",
        "# Compute the classification metrics\n",
        "accuracy = accuracy_score(y_test, predictions)\n",
        "precision = precision_score(y_test, predictions, average='macro')\n",
        "recall = recall_score(y_test, predictions, average='macro')\n",
        "f1 = f1_score(y_test, predictions, average='macro')\n",
        "print('Accuracy:',accuracy.round(4))\n",
        "print('Precision:',precision.round(4))\n",
        "print('Recall:',recall.round(4))\n",
        "print('F1:',f1.round(4))\n",
        "\n",
        "# Plot the confusion matrix\n",
        "plt.figure(figsize=(10,8))\n",
        "sns.heatmap(cm.T, xticklabels=list(labels), yticklabels=list(labels))\n",
        "plt.xlabel('True labels')\n",
        "plt.ylabel('Predicted labels')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EPTEvR1UQ9Q_"
      },
      "source": [
        "# Visualize the activations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mLy0-hpxuf6_"
      },
      "outputs": [],
      "source": [
        "# Get sample batch\n",
        "batch = next(test_gen)[0]\n",
        "\n",
        "# Get first image\n",
        "image = batch[0] # batch size = 8\n",
        "\n",
        "fig = plt.figure(figsize=(6, 4))\n",
        "plt.imshow(np.uint8(image))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iNckjPfV_x0Q"
      },
      "outputs": [],
      "source": [
        "# Get the activations (the output of each ReLU layer)\n",
        "# We can do it by creating a new Model (activation_model) with the same input as \n",
        "# the original model and all the ReLU activations as output\n",
        "layers = [layer.output for layer in supernet.layers if isinstance(layer, tfk.layers.Conv2D)]\n",
        "\n",
        "print(len(layers))\n",
        "activation_model = tf.keras.Model(inputs=supernet.input, outputs=layers)\n",
        "\n",
        "# Finally we get the output feature maps (for each layer) given the imput test image\n",
        "fmaps = activation_model.predict(tf.expand_dims(image, 0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Ighu54T_1kk"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.axes_grid1 import ImageGrid\n",
        "%matplotlib inline\n",
        "def display_activation(fmaps, depth=0, first_n=-1): \n",
        "    # fmaps: list of all the feature maps for each layer\n",
        "    # depth: the layer we want to visualize (an int in [0, network depth))\n",
        "    # first_n: default '-1' means 'all activations'. Number of activations to be visualized. Note that for deep layers it could be a large number.\n",
        "\n",
        "    fmaps = fmaps[depth] # get feature maps at the desired depth\n",
        "    if first_n > 0:\n",
        "      fmaps = fmaps[0, :, :, :first_n] \n",
        "    fmaps = tf.image.resize(fmaps, size=[128, 128]) # resize for visualization\n",
        "\n",
        "    # Distribute on a grid for plotting\n",
        "    col_size = 8\n",
        "    row_size = fmaps.shape[-1] // 8\n",
        "    fmap_channel=0\n",
        "    fig = plt.figure(figsize=(30, 30))\n",
        "    grid = ImageGrid(fig, 111,  # similar to subplot(111)\n",
        "                    nrows_ncols=(row_size, col_size),  \n",
        "                    axes_pad=0.1,  # pad between axes in inch.\n",
        "                    )\n",
        "    for row in range(0,row_size):\n",
        "        for col in range(0,col_size):\n",
        "            grid[fmap_channel].imshow(fmaps[0, :, :, fmap_channel], cmap='gray', aspect='auto')\n",
        "            fmap_channel += 1\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_FlfBw24_5_D"
      },
      "outputs": [],
      "source": [
        "for i in range(0, 4, 2):\n",
        "    print(i)\n",
        "    display_activation(fmaps=fmaps, depth=i, first_n=-1)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "base_model.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
